{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from glob import glob\npathes = glob(\"/kaggle/input/imagenetmini-1000/imagenet-mini/train//**/*.JPEG\", recursive=True)\nprint(len(pathes))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:33:18.800708Z","iopub.execute_input":"2024-04-14T10:33:18.801078Z","iopub.status.idle":"2024-04-14T10:33:19.824336Z","shell.execute_reply.started":"2024-04-14T10:33:18.801050Z","shell.execute_reply":"2024-04-14T10:33:19.822956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom glob import glob","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InpaintingDataset(Dataset):\n    def __init__(self, root_dir, transform=None, mask_size=100, n_masks=1):\n        \"\"\"\n        Dataset for image inpainting.\n\n        Args:\n        - root_dir (str): Root directory containing images.\n        - transform (callable, optional): A function/transform to apply to the images.\n        - mask_size (int, optional): Size of the square mask to apply.\n        - n_masks (int, optional): Number of masks to apply per image.\n        \"\"\"\n        self.image_paths = glob(root_dir + \"/**/*.JPEG\", recursive=True)\n        self.transform = transform\n        self.mask_size = mask_size\n        self.n_masks = n_masks\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Get a sample from the dataset.\n\n        Args:\n        - index (int): Index of the sample to retrieve.\n\n        Returns:\n        - image (Tensor): Original image.\n        - masked_img (Tensor): Image with random masks applied.\n        - mask (Tensor): Binary mask indicating masked regions.\n        - mask_region (Tensor): Coordinates of the mask regions.\n        \"\"\"\n        image_path = self.image_paths[index]\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        masked_img, mask, mask_region = self.apply_random_mask(image, self.n_masks)\n\n        return image, masked_img, mask, mask_region\n    \n    def apply_random_mask(self, img, n_masks=1):\n        \"\"\"\n        Apply random masks to the input image.\n\n        Args:\n        - img (Tensor): Input image.\n        - n_masks (int, optional): Number of masks to apply.\n\n        Returns:\n        - masked_img (Tensor): Image with random masks applied.\n        - mask (Tensor): Binary mask indicating masked regions.\n        - mask_region (Tensor): Coordinates of the mask regions.\n        \"\"\"\n        masked_img = img.clone()\n        fill_value = masked_img.mean()\n        mask = torch.zeros(masked_img.shape)\n        \n        y1 = torch.randint(0, masked_img.shape[1] - self.mask_size, (n_masks, ))\n        x1 = torch.randint(0, masked_img.shape[2] - self.mask_size, (n_masks, ))\n        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n        \n        for i in range(n_masks):\n            masked_img[:, y1[i]:y2[i], x1[i]:x2[i]] = fill_value\n            mask[:, y1[i]:y2[i], x1[i]:x2[i]] = 1\n        \n        return masked_img, mask, torch.stack((y1, x1), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:50:07.708760Z","iopub.execute_input":"2024-04-14T10:50:07.709158Z","iopub.status.idle":"2024-04-14T10:50:07.720831Z","shell.execute_reply.started":"2024-04-14T10:50:07.709131Z","shell.execute_reply":"2024-04-14T10:50:07.719608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = \"/kaggle/input/imagenetmini-1000/imagenet-mini/train\"\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n])\ndataset = InpaintingDataset(root_dir, transform, mask_size=50, n_masks=2)\nlen(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:50:10.064755Z","iopub.execute_input":"2024-04-14T10:50:10.065427Z","iopub.status.idle":"2024-04-14T10:50:11.098766Z","shell.execute_reply.started":"2024-04-14T10:50:10.065383Z","shell.execute_reply":"2024-04-14T10:50:11.097822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, masked_img, mask, masks_coords = dataset[0]\nimage.shape, masked_img.shape, mask.shape, masks_coords.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:57:05.966700Z","iopub.execute_input":"2024-04-14T10:57:05.967134Z","iopub.status.idle":"2024-04-14T10:57:05.985319Z","shell.execute_reply.started":"2024-04-14T10:57:05.967103Z","shell.execute_reply":"2024-04-14T10:57:05.984040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 3, 1)\nplt.imshow(image.permute(1, 2, 0).numpy())\nplt.subplot(1, 3, 2)\nplt.imshow(masked_img.permute(1, 2, 0).numpy())\nplt.subplot(1, 3, 3)\nplt.imshow(mask.permute(1, 2, 0).numpy())","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:57:52.190178Z","iopub.execute_input":"2024-04-14T10:57:52.190582Z","iopub.status.idle":"2024-04-14T10:57:52.777980Z","shell.execute_reply.started":"2024-04-14T10:57:52.190536Z","shell.execute_reply":"2024-04-14T10:57:52.776795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_masked_region(image, masks_coords, mask_size):\n    \"\"\"\n    Extract masked regions from the input image based on the provided coordinates.\n\n    Args:\n    - image (Tensor): Input image.\n    - masks_coords (Tensor): Coordinates of the masked regions.\n    - mask_size (int): Size of the square mask.\n\n    Returns:\n    - regions (list): List of masked regions extracted from the image.\n    \"\"\"\n    regions = []\n    for y1, x1 in masks_coords:\n        regions.append(image[:, y1:y1+mask_size, x1:x1+mask_size])\n    return regions","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:57:39.915330Z","iopub.execute_input":"2024-04-14T10:57:39.916673Z","iopub.status.idle":"2024-04-14T10:57:39.923271Z","shell.execute_reply.started":"2024-04-14T10:57:39.916622Z","shell.execute_reply":"2024-04-14T10:57:39.922356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regions = get_masked_region(image, masks_coords, 50)\nfor i in range(len(regions)):\n    plt.subplot(1, 2, i + 1)\n    plt.imshow(regions[i].permute(1, 2, 0).numpy())","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:57:46.761533Z","iopub.execute_input":"2024-04-14T10:57:46.761971Z","iopub.status.idle":"2024-04-14T10:57:47.132930Z","shell.execute_reply.started":"2024-04-14T10:57:46.761939Z","shell.execute_reply":"2024-04-14T10:57:47.131787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Completion Class","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    \"\"\"\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"\n        Initialize Generator model\n        \"\"\"\n        super(CompletionNetwork, self).__init__()\n        conv_block = lambda in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1: [\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        ]\n\n        deconv_block = lambda in_channels, out_channels, kernel_size, stride=1, padding=0: [\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        ]\n        \n        self.encoder = nn.Sequential(\n            *conv_block(4, 64, kernel_size=5, padding=2),\n            *conv_block(64, 128, kernel_size=3, stride=2, padding=1),\n            *conv_block(128, 128, kernel_size=3, padding=1),\n            *conv_block(128, 256, kernel_size=3, stride=2, padding=1),\n            *conv_block(256, 256, kernel_size=3, padding=1),\n            *conv_block(256, 256, kernel_size=3, padding=1),\n            *conv_block(256, 256, kernel_size=3, dilation=2, padding=2),\n            *conv_block(256, 256, kernel_size=3, dilation=4, padding=4),\n            *conv_block(256, 256, kernel_size=3, dilation=8, padding=8),\n            *conv_block(256, 256, kernel_size=3, dilation=16, padding=16),\n            *conv_block(256, 256, kernel_size=3, padding=1),\n            *conv_block(256, 256, kernel_size=3, padding=1)\n        )\n        \n        self.decoder = nn.Sequential(\n            *deconv_block(256, 128, kernel_size=4, stride=2, padding=1),\n            *conv_block(128, 128, kernel_size=3, padding=1),\n            *deconv_block(128, 64, kernel_size=4, stride=2, padding=1),\n            *conv_block(64, 32, kernel_size=3, padding=1),\n            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n            nn.Sigmoid()\n        )\n        \n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        output = self.decoder(encoded)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global discriminator class","metadata":{}},{"cell_type":"code","source":"class GlobalDiscriminator(nn.Module):\n    \"\"\"\n    \"\"\"\n    \n    def __init__(self, image_shape):\n        \"\"\"\n        Initialize Global Discriminator model\n        \"\"\"\n        conv_block = lambda in_channels, out_channels, kernel_size=5, stride=2, padding=2: [\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        ]\n        \n        self.layers = nn.Sequential(\n            *conv_block(in_channels=image_shape[0], out_channels=64),\n            *conv_block(in_channels=64, out_channels=128),\n            *conv_block(in_channels=128, out_channels=256),\n            *conv_block(in_channels=256, out_channels=512),\n            *conv_block(in_channels=512, out_channels=512),\n            *conv_block(in_channels=512, out_channels=512),\n        )\n        \n        self.flatten_layer = nn.Flatten()\n        out_h = image_shape[1] // (2 ** 6)  # 6 max-pooling layers with stride 2\n        out_w = image_shape[2] // (2 ** 6)\n        self.linear = nn.Linear(512 * out_h * out_w, 1024)\n        self.activation = nn.ReLU()\n    \n    def forward(self, x):\n        conv_output = self.layers(x)\n        flatten_output = self.flatten_layer(conv_output)\n        output = self.activation(self.linear(flatten_output))\n        return output\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Local discriminator class","metadata":{}},{"cell_type":"code","source":"class LocalDiscriminator(nn.Module):\n    \"\"\"\n    \"\"\"\n    \n    def __init__(self, image_shape):\n        \"\"\"\n        Initialize Local Discriminator model\n        \"\"\"\n        \n        conv_block = lambda in_channels, out_channels, kernel_size=5, stride=2, padding=2: [\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        ]\n        \n        self.layers = nn.Sequential(\n            *conv_block(in_channels=image_shape[0], out_channels=64),\n            *conv_block(in_channels=64, out_channels=128),\n            *conv_block(in_channels=128, out_channels=256),\n            *conv_block(in_channels=256, out_channels=512),\n            *conv_block(in_channels=512, out_channels=512),\n        )\n        \n        self.flatten_layer = nn.Flatten()\n        out_h = image_shape[1] // (2 ** 5)  # 5 max-pooling layers with stride 2\n        out_w = image_shape[2] // (2 ** 5)\n        self.linear = nn.Linear(512 * out_h * out_w, 1024)\n        self.activation = nn.ReLU()\n        \n    \n    def forward(self, x):\n        conv_output = self.layers(x)\n        flatten_output = self.flatten_layer(conv_output)\n        output = self.activation(self.linear(flatten_output))\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Context discriminator class","metadata":{}},{"cell_type":"code","source":"class ConcatedDiscriminator(nn.Module):\n    \"\"\"\n    \"\"\"\n    def __init__(self, local_input_shape, global_input_shape):\n        \"\"\"\n        Initialize Context Discriminator model\n        \"\"\"\n        \n        self.local_discrimitator = LocalDiscriminator(local_input_shape)\n        self.global_discrimitator = GlobalDiscriminator(global_input_shape)\n        \n        self.concatenation_layer = nn.Concatenate(dim=-1)\n        self.linear_layer = nn.Linear(2048, 1)\n        self.sigmoid_layer = nn.Sigmoid()\n    \n    def forward(self, x):\n        local_discriminator_output =  self.local_discrimitator(x)\n        global_discrimitator_output =  self.global_discrimitator(x)\n        \n        concatenated_output = self.concatenation_layer([local_discriminator_output, global_discrimitator_output])\n        output = self.sigmoid_layer(self.linear_layer(concatenated_output))\n        return output\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train code","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}