{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:33.486979Z","iopub.status.busy":"2024-05-14T13:49:33.486629Z","iopub.status.idle":"2024-05-14T13:49:39.666172Z","shell.execute_reply":"2024-05-14T13:49:39.665133Z","shell.execute_reply.started":"2024-05-14T13:49:33.486949Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from glob import glob\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle"]},{"cell_type":"markdown","metadata":{},"source":["# Constants"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:39.668594Z","iopub.status.busy":"2024-05-14T13:49:39.668160Z","iopub.status.idle":"2024-05-14T13:49:39.673115Z","shell.execute_reply":"2024-05-14T13:49:39.672020Z","shell.execute_reply.started":"2024-05-14T13:49:39.668567Z"},"trusted":true},"outputs":[],"source":["MASK_SIZE = 50\n","N_MASKS = 2\n","FILL_VALUE = 0.5\n","UNET_FLAG = False"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:39.674301Z","iopub.status.busy":"2024-05-14T13:49:39.674005Z","iopub.status.idle":"2024-05-14T13:49:39.691596Z","shell.execute_reply":"2024-05-14T13:49:39.690806Z","shell.execute_reply.started":"2024-05-14T13:49:39.674277Z"},"trusted":true},"outputs":[],"source":["class InpaintingDataset(Dataset):\n","    def __init__(self, image_paths, transform=None, mask_size=100, n_masks=1):\n","        \"\"\"\n","        Dataset for image inpainting.\n","\n","        Args:\n","        - image_paths (List[str]): List of images paths\n","        - transform (callable, optional): A function/transform to apply to the images.\n","        - mask_size (int, optional): Size of the square mask to apply.\n","        - n_masks (int, optional): Number of masks to apply per image.\n","        \"\"\"\n","        self.image_paths = image_paths\n","        self.transform = transform\n","        self.mask_size = mask_size\n","        self.n_masks = n_masks\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, index):\n","        image_path = self.image_paths[index]\n","        image = Image.open(image_path).convert('RGB')\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        mask, mask_region = self.apply_random_mask(image, self.n_masks)\n","\n","        return image, mask, mask_region\n","\n","    def apply_random_mask(self, img, n_masks=1):\n","        \"\"\"\n","        Apply random masks to the input image, ensuring no overlap.\n","\n","        Args:\n","        - img (Tensor): Input image.\n","        - n_masks (int, optional): Number of masks to apply.\n","\n","        Returns:\n","        - mask (Tensor): Binary mask indicating masked regions.\n","        - mask_region (Tensor): Coordinates of the mask regions.\n","        \"\"\"\n","        mask = torch.zeros(img.shape[1:]).unsqueeze(0)\n","        mask_coords = []\n","        \n","        for _ in range(n_masks):\n","            intersects = True\n","            while intersects:\n","                y1 = torch.randint(0, img.shape[1] - self.mask_size, (1,)).item()\n","                x1 = torch.randint(0, img.shape[2] - self.mask_size, (1,)).item()\n","                y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n","\n","                intersects = any(\n","                    y1 < y2p and y2 > y1p and x1 < x2p and x2 > x1p\n","                    for y1p, x1p, y2p, x2p in mask_coords\n","                )\n","\n","            mask[:, y1:y2, x1:x2] = 1\n","            mask_coords.append((y1, x1, y2, x2))\n","\n","        mask_regions = torch.tensor([[y1, x1] for y1, x1, y2, x2 in mask_coords])\n","        return mask, mask_regions"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:39.692810Z","iopub.status.busy":"2024-05-14T13:49:39.692564Z","iopub.status.idle":"2024-05-14T13:49:55.266334Z","shell.execute_reply":"2024-05-14T13:49:55.265407Z","shell.execute_reply.started":"2024-05-14T13:49:39.692788Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(10000, 2899, 1024)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import random\n","train_path = \"/kaggle/input/imagenetmini-1000/imagenet-mini/train\"\n","val_path = \"/kaggle/input/imagenetmini-1000/imagenet-mini/val\"\n","\n","train_paths_list = glob(train_path + \"/**/*.JPEG\", recursive=True)[:10000]\n","val_paths_list = glob(val_path + \"/**/*.JPEG\", recursive=True)\n","# split validation data on val and test\n","random.shuffle(val_paths_list)\n","val_paths_list, test_paths_list = val_paths_list[:-1024], val_paths_list[-1024:]\n","\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = InpaintingDataset(train_paths_list, transform, mask_size=MASK_SIZE, n_masks=N_MASKS)\n","val_dataset = InpaintingDataset(val_paths_list, transform, mask_size=MASK_SIZE, n_masks=N_MASKS)\n","test_dataset = InpaintingDataset(test_paths_list, transform, mask_size=MASK_SIZE, n_masks=N_MASKS)\n","len(train_dataset), len(val_dataset), len(test_dataset)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:55.269841Z","iopub.status.busy":"2024-05-14T13:49:55.269416Z","iopub.status.idle":"2024-05-14T13:49:55.362353Z","shell.execute_reply":"2024-05-14T13:49:55.361457Z","shell.execute_reply.started":"2024-05-14T13:49:55.269814Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(torch.Size([3, 256, 256]), torch.Size([1, 256, 256]), torch.Size([2, 2]))"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["image, mask, masks_coords = train_dataset[0]\n","image.shape, mask.shape, masks_coords.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:55.363882Z","iopub.status.busy":"2024-05-14T13:49:55.363519Z","iopub.status.idle":"2024-05-14T13:49:55.967436Z","shell.execute_reply":"2024-05-14T13:49:55.966588Z","shell.execute_reply.started":"2024-05-14T13:49:55.363851Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 5))\n","plt.subplot(1, 3, 1)\n","plt.imshow(image.permute(1, 2, 0).numpy())\n","plt.subplot(1, 3, 2)\n","masked_img = image - image * mask + FILL_VALUE * mask\n","plt.imshow(masked_img.permute(1, 2, 0).numpy())\n","plt.subplot(1, 3, 3)\n","plt.imshow(mask.permute(1, 2, 0).numpy(), cmap=\"binary\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:55.968928Z","iopub.status.busy":"2024-05-14T13:49:55.968635Z","iopub.status.idle":"2024-05-14T13:49:55.974586Z","shell.execute_reply":"2024-05-14T13:49:55.973620Z","shell.execute_reply.started":"2024-05-14T13:49:55.968904Z"},"trusted":true},"outputs":[],"source":["def get_masked_region(image, masks_coords, mask_size):\n","    \"\"\"\n","    Extract masked regions from the input image based on the provided coordinates.\n","\n","    Args:\n","    - image (Tensor): Input image.\n","    - masks_coords (Tensor): Coordinates of the masked regions.\n","    - mask_size (int): Size of the square mask.\n","\n","    Returns:\n","    - regions (list): List of masked regions extracted from the image.\n","    \"\"\"\n","    regions = []\n","    for y1, x1 in masks_coords:\n","        regions.append(image[:, y1:y1+mask_size, x1:x1+mask_size])\n","    return regions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:55.975804Z","iopub.status.busy":"2024-05-14T13:49:55.975557Z","iopub.status.idle":"2024-05-14T13:49:56.339984Z","shell.execute_reply":"2024-05-14T13:49:56.339103Z","shell.execute_reply.started":"2024-05-14T13:49:55.975778Z"},"trusted":true},"outputs":[],"source":["regions = get_masked_region(image, masks_coords, 50)\n","for i in range(len(regions)):\n","    plt.subplot(1, 2, i + 1)\n","    plt.imshow(regions[i].permute(1, 2, 0).numpy())"]},{"cell_type":"markdown","metadata":{},"source":["# Models classes"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:56.341469Z","iopub.status.busy":"2024-05-14T13:49:56.341149Z","iopub.status.idle":"2024-05-14T13:49:56.345610Z","shell.execute_reply":"2024-05-14T13:49:56.344754Z","shell.execute_reply.started":"2024-05-14T13:49:56.341442Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.nn.functional import mse_loss\n","from tqdm.notebook import trange, tqdm\n","from torch.optim import Adam, Adadelta\n","from torch.nn import BCELoss, MSELoss"]},{"cell_type":"markdown","metadata":{},"source":["## Completion Class"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:56.347379Z","iopub.status.busy":"2024-05-14T13:49:56.347051Z","iopub.status.idle":"2024-05-14T13:49:56.360654Z","shell.execute_reply":"2024-05-14T13:49:56.359816Z","shell.execute_reply.started":"2024-05-14T13:49:56.347350Z"},"trusted":true},"outputs":[],"source":["class Generator(nn.Module):\n","    \n","    def __init__(self):\n","        \"\"\"\n","        Initialize Generator model\n","        \"\"\"\n","        super(Generator, self).__init__()\n","        conv_block = lambda in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1: [\n","            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU()\n","        ]\n","\n","        deconv_block = lambda in_channels, out_channels, kernel_size, stride=1, padding=0: [\n","            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU()\n","        ]\n","        \n","        self.encoder = nn.Sequential(\n","            *conv_block(4, 64, kernel_size=5, padding=2),\n","            *conv_block(64, 128, kernel_size=3, stride=2, padding=1),\n","            *conv_block(128, 128, kernel_size=3, padding=1),\n","            *conv_block(128, 256, kernel_size=3, stride=2, padding=1),\n","            *conv_block(256, 256, kernel_size=3, padding=1),\n","            *conv_block(256, 256, kernel_size=3, padding=1),\n","            *conv_block(256, 256, kernel_size=3, dilation=2, padding=2),\n","            *conv_block(256, 256, kernel_size=3, dilation=4, padding=4),\n","            *conv_block(256, 256, kernel_size=3, dilation=8, padding=8),\n","            *conv_block(256, 256, kernel_size=3, dilation=16, padding=16),\n","            *conv_block(256, 256, kernel_size=3, padding=1),\n","            *conv_block(256, 256, kernel_size=3, padding=1)\n","        )\n","        \n","        self.decoder = nn.Sequential(\n","            *deconv_block(256, 128, kernel_size=4, stride=2, padding=1),\n","            *conv_block(128, 128, kernel_size=3, padding=1),\n","            *deconv_block(128, 64, kernel_size=4, stride=2, padding=1),\n","            *conv_block(64, 32, kernel_size=3, padding=1),\n","            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n","            nn.Sigmoid()\n","        )\n","        \n","        \n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        output = self.decoder(encoded)\n","        return output"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:56.362070Z","iopub.status.busy":"2024-05-14T13:49:56.361775Z","iopub.status.idle":"2024-05-14T13:49:56.376673Z","shell.execute_reply":"2024-05-14T13:49:56.375895Z","shell.execute_reply.started":"2024-05-14T13:49:56.362048Z"},"trusted":true},"outputs":[],"source":["class UNetGenerator(nn.Module):\n","\n","    def __init__(self):\n","        super(UNetGenerator, self).__init__()\n","\n","        def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n","            return nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(inplace=True)\n","            )\n","\n","        def deconv_block(in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(inplace=True)\n","            )\n","        \n","        self.enc1 = conv_block(4, 64)\n","        self.enc2 = conv_block(64, 128)\n","        self.enc3 = conv_block(128, 256)\n","        self.enc4 = conv_block(256, 512)\n","        self.enc5 = conv_block(512, 1024)\n","\n","        self.dec4 = deconv_block(1024, 512)\n","        self.dec3 = deconv_block(1024, 256)\n","        self.dec2 = deconv_block(512, 128)\n","        self.dec1 = deconv_block(256, 64)\n","        \n","        self.final = nn.Sequential(\n","            nn.Conv2d(128, 32, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(32, 3, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        e1 = self.enc1(x)\n","        e2 = self.enc2(F.max_pool2d(e1, 2))\n","        e3 = self.enc3(F.max_pool2d(e2, 2))\n","        e4 = self.enc4(F.max_pool2d(e3, 2))\n","        e5 = self.enc5(F.max_pool2d(e4, 2))\n","        \n","        d4 = self.dec4(e5)\n","        d3 = self.dec3(torch.cat([d4, e4], dim=1))\n","        d2 = self.dec2(torch.cat([d3, e3], dim=1))\n","        d1 = self.dec1(torch.cat([d2, e2], dim=1))\n","        output = self.final(torch.cat([d1, e1], dim=1))\n","        \n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["## Global discriminator class"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:56.377932Z","iopub.status.busy":"2024-05-14T13:49:56.377676Z","iopub.status.idle":"2024-05-14T13:49:56.390045Z","shell.execute_reply":"2024-05-14T13:49:56.389314Z","shell.execute_reply.started":"2024-05-14T13:49:56.377910Z"},"trusted":true},"outputs":[],"source":["class GlobalDiscriminator(nn.Module):\n","    \n","    def __init__(self, image_shape):\n","        super(GlobalDiscriminator, self).__init__()\n","        \"\"\"\n","        Initialize Global Discriminator model\n","        \"\"\"\n","        conv_block = lambda in_channels, out_channels, kernel_size=5, stride=2, padding=2: [\n","            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","        ]\n","        \n","        self.layers = nn.Sequential(\n","            *conv_block(in_channels=image_shape[0], out_channels=64),\n","            *conv_block(in_channels=64, out_channels=128),\n","            *conv_block(in_channels=128, out_channels=256),\n","            *conv_block(in_channels=256, out_channels=512),\n","            *conv_block(in_channels=512, out_channels=512),\n","            *conv_block(in_channels=512, out_channels=512),\n","        )\n","        \n","        self.flatten_layer = nn.Flatten()\n","        out_h = image_shape[1] // (2 ** 6)\n","        out_w = image_shape[2] // (2 ** 6)\n","        self.linear = nn.Linear(512 * out_h * out_w, 1024)\n","        self.activation = nn.ReLU()\n","    \n","    def forward(self, x):\n","        conv_output = self.layers(x)\n","        flatten_output = self.flatten_layer(conv_output)\n","        output = self.activation(self.linear(flatten_output))\n","        return output\n"]},{"cell_type":"markdown","metadata":{},"source":["## Local discriminator class"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:56.393535Z","iopub.status.busy":"2024-05-14T13:49:56.392784Z","iopub.status.idle":"2024-05-14T13:49:56.403846Z","shell.execute_reply":"2024-05-14T13:49:56.403064Z","shell.execute_reply.started":"2024-05-14T13:49:56.393501Z"},"trusted":true},"outputs":[],"source":["class LocalDiscriminator(nn.Module):\n","    \n","    def __init__(self, image_shape):\n","        \"\"\"\n","        Initialize Local Discriminator model\n","        \"\"\"\n","        print(image_shape)\n","        super(LocalDiscriminator, self).__init__()\n","        conv_block = lambda in_channels, out_channels, kernel_size=5, stride=2, padding=2: [\n","            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","        ]\n","        \n","        self.layers = nn.Sequential(\n","            *conv_block(in_channels=image_shape[0], out_channels=64),\n","            *conv_block(in_channels=64, out_channels=128),\n","            *conv_block(in_channels=128, out_channels=256),\n","            *conv_block(in_channels=256, out_channels=512),\n","            *conv_block(in_channels=512, out_channels=512),\n","        )\n","        \n","        self.flatten_layer = nn.Flatten()\n","        out_h = image_shape[1] // (2 ** 5)\n","        out_w = image_shape[2] // (2 ** 5)\n","        self.linear = nn.Linear(512 * out_h * out_w, 1024)\n","        self.activation = nn.ReLU()\n","        \n","    \n","    def forward(self, x):\n","        conv_output = self.layers(x)\n","        flatten_output = self.flatten_layer(conv_output)\n","        output = self.activation(self.linear(flatten_output))\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["## Context discriminator class"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:56.408507Z","iopub.status.busy":"2024-05-14T13:49:56.407779Z","iopub.status.idle":"2024-05-14T13:49:56.417195Z","shell.execute_reply":"2024-05-14T13:49:56.416531Z","shell.execute_reply.started":"2024-05-14T13:49:56.408479Z"},"trusted":true},"outputs":[],"source":["class ContextDiscriminator(nn.Module):\n","    \n","    def __init__(self, local_input_shape, global_input_shape):\n","        \"\"\"\n","        Initialize Context Discriminator model\n","        \"\"\"\n","        super(ContextDiscriminator, self).__init__()\n","        self.local_discrimitator = LocalDiscriminator(local_input_shape)\n","        self.global_discrimitator = GlobalDiscriminator(global_input_shape)\n","        \n","        self.linear_layer = nn.Linear(2048, 1)\n","        self.sigmoid_layer = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        x_global, x_local = x\n","        local_discriminator_output =  self.local_discrimitator(x_local)\n","        global_discrimitator_output =  self.global_discrimitator(x_global)\n","        global_discrimitator_output = global_discrimitator_output.repeat(N_MASKS, 1)\n","        \n","        concatenated_output = torch.cat([local_discriminator_output, global_discrimitator_output], dim=-1)\n","        output = self.sigmoid_layer(self.linear_layer(concatenated_output))\n","        return output\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Train code"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:56.418425Z","iopub.status.busy":"2024-05-14T13:49:56.418148Z","iopub.status.idle":"2024-05-14T13:49:56.430622Z","shell.execute_reply":"2024-05-14T13:49:56.429814Z","shell.execute_reply.started":"2024-05-14T13:49:56.418404Z"},"trusted":true},"outputs":[],"source":["def get_masked_regions_batched(images, masks_coords, MASK_SIZE=50):\n","    MASK_SIZE = min(MASK_SIZE, 50)\n","    padded_size = 64\n","    pad_half = (padded_size - MASK_SIZE) // 2\n","\n","    masked_regions = []\n","    N_MASKS = masks_coords.shape[1]\n","\n","    for mask_ind in range(N_MASKS):\n","        y1 = masks_coords[:, mask_ind, 0].cpu()\n","        x1 = masks_coords[:, mask_ind, 1].cpu()\n","\n","        for img, y, x in zip(images, y1, x1):\n","            y_min = max(0, y - pad_half)\n","            y_max = min(img.shape[1], y + MASK_SIZE + pad_half)\n","            x_min = max(0, x - pad_half)\n","            x_max = min(img.shape[2], x + MASK_SIZE + pad_half)\n","\n","            region = img[:, y_min:y_max, x_min:x_max].to(img.device)\n","            padded_region = torch.zeros((img.shape[0], padded_size, padded_size), dtype=img.dtype, device=img.device)\n","            y_start = max(0, pad_half - (y - y_min))\n","            x_start = max(0, pad_half - (x - x_min))\n","            y_end = y_start + min(region.shape[1], padded_size)\n","            x_end = x_start + min(region.shape[2], padded_size)\n","\n","            padded_region[:, y_start:y_end, x_start:x_end] = region\n","\n","            masked_regions.append(padded_region)\n","\n","    return torch.stack(masked_regions)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:56.444720Z","iopub.status.busy":"2024-05-14T13:49:56.444434Z","iopub.status.idle":"2024-05-14T13:49:56.469364Z","shell.execute_reply":"2024-05-14T13:49:56.468576Z","shell.execute_reply.started":"2024-05-14T13:49:56.444697Z"},"trusted":true},"outputs":[],"source":["class Trainer():\n","    def __init__(self, generator, context_discriminator, config, \n","                 train_data, val_data, test_data):\n","        \"\"\"\n","        Trainer class for training a generative model and a context discriminator.\n","        \"\"\"\n","        self.generator = generator\n","        self.context_disc = context_discriminator\n","        self.device = config[\"device\"]\n","        \n","        self.optimizer_gen = Adam(generator.parameters(), lr=config[\"generator_lr\"])\n","        self.optimizer_disc = Adam(context_discriminator.parameters(), lr=config[\"discriminator_lr\"])\n","        \n","        self.train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], \n","                                       shuffle=True, pin_memory=True, num_workers=4)\n","        self.val_loader = DataLoader(val_data, batch_size=config[\"batch_size\"], \n","                                     shuffle=False, pin_memory=True, num_workers=4)\n","        self.test_data = DataLoader(test_data, batch_size=config[\"batch_size\"], \n","                                    shuffle=False, pin_memory=True, num_workers=4)\n","        \n","        self.disc_loss = BCELoss()\n","        self.alpha = config[\"alpha\"]\n","        self.logs = {\n","            \"train_loss_gen\": [],\n","            \"train_loss_disc\" : [],\n","            \"val_loss_gen\": []\n","        }\n","        \n","    def train_generator_step(self, batch, train_disc):\n","        images, masks, masks_coords = batch\n","        masked_img = images * (1 - masks) + FILL_VALUE * masks\n","        gen_input = torch.cat((masked_img, masks), dim=1)\n","        out_gen = self.generator(gen_input)\n","        loss = mse_loss(out_gen, images)\n","\n","        if train_disc:\n","            fake_input_disc_local = get_masked_regions_batched(out_gen, masks_coords)\n","            out_disc_fake = self.context_disc((out_gen, fake_input_disc_local))\n","            real_labels = torch.ones((len(images), 1), device=self.device)\n","            loss_disc = self.disc_loss(out_disc_fake, real_labels)\n","            loss += self.alpha * loss_disc\n","\n","        return loss, out_gen.detach()\n","\n","    \n","    def train_disc_only_step(self, batch):\n","        \"\"\"\n","        Perform a training step for the discriminator only.\n","\n","        Args:\n","            batch (tuple): A batch of data containing images, masks, mask coordinates, and completed images.\n","\n","        Returns:\n","            torch.Tensor: The discriminator loss.\n","        \"\"\"\n","        images, masks, masks_coords, completed_image = batch\n","        fake_input_disc_local = get_masked_regions_batched(completed_image.detach(), masks_coords)\n","        out_disc_fake = self.context_disc((completed_image.detach(), fake_input_disc_local))\n","        fake_labels = torch.zeros((len(images), 1), device=self.device)\n","        loss_fake = self.disc_loss(out_disc_fake, fake_labels)\n","\n","        real_input_disc_local = get_masked_regions_batched(images, masks_coords)\n","        out_disc_real = self.context_disc((images, real_input_disc_local))\n","        real_labels = torch.ones((len(images), 1), device=self.device)\n","        loss_real = self.disc_loss(out_disc_real, real_labels)\n","\n","        loss = (loss_fake + loss_real) * self.alpha / 2\n","        return loss\n","        \n","    def eval_generator(self, with_adversarial=False):\n","        self.generator.eval()\n","        self.context_disc.eval()\n","        val_loss = 0.0\n","        for batch in tqdm(self.val_loader, desc=\"Evaluation\"):\n","            images, masks, masks_coords = batch\n","            images, masks, masks_coords = images.to(self.device), masks.to(self.device), masks_coords.to(self.device)\n","            with torch.no_grad():\n","                loss_gen, _ = self.train_generator_step((images, masks, masks_coords),\n","                                                                  train_disc=with_adversarial)\n","            val_loss += loss_gen.data.item()\n","        return val_loss / len(self.val_loader)\n","    \n","    def train(self, epochs, train_disc=3):\n","        \"\"\"\n","        Train the generator and discriminator models for a given number of epochs.\n","        \"\"\"\n","        for epoch in range(epochs):\n","            train_loss_gen = 0.0\n","            train_loss_disc = 0.0\n","            self.generator.train()\n","            self.context_disc.train()\n","            for batch in tqdm(self.train_loader, desc=\"Training\"):\n","                images, masks, masks_coords = batch\n","                images, masks, masks_coords = images.to(self.device), masks.to(self.device), masks_coords.to(self.device)\n","                \n","                loss_gen, completed_image = self.train_generator_step((images, masks, masks_coords),\n","                                                                      train_disc=(epoch >= train_disc))\n","                loss_gen.backward()\n","                self.optimizer_gen.step()\n","                self.optimizer_gen.zero_grad()\n","                train_loss_gen += loss_gen.data.item()\n","                \n","                if epoch >= train_disc:\n","                    self.optimizer_disc.zero_grad()\n","                    loss_disc = self.train_disc_only_step((images, masks, masks_coords, completed_image))\n","                    loss_disc.backward()\n","                    self.optimizer_disc.step()\n","                    train_loss_disc += loss_disc.data.item()\n","            \n","            train_loss_gen /= len(self.train_loader)\n","            train_loss_disc /= len(self.train_loader)\n","            \n","            val_loss_gen = self.eval_generator(with_adversarial = (epoch >= train_disc))\n","            print(f\"Epoch: {epoch} | Train gen loss: {train_loss_gen:.5f}| Train disc loss: {train_loss_disc:.5f} | Eval loss: {val_loss_gen:.5f}\")\n","            \n","            self.logs[\"train_loss_gen\"].append(train_loss_gen)\n","            self.logs[\"train_loss_disc\"].append(train_loss_disc)\n","            self.logs[\"val_loss_gen\"].append(val_loss_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:56.471132Z","iopub.status.busy":"2024-05-14T13:49:56.470529Z","iopub.status.idle":"2024-05-14T13:49:57.138426Z","shell.execute_reply":"2024-05-14T13:49:57.137400Z","shell.execute_reply.started":"2024-05-14T13:49:56.471102Z"},"trusted":true},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)\n","print(\"Using UNet Generator:\", UNET_FLAG)\n","generator = UNetGenerator().to(device) if UNET_FLAG else Generator().to(device)\n","context_discriminator = ContextDiscriminator((3, 64, 64), (3, 256, 256)).to(device)\n","config = {\n","    \"device\": device,\n","    \"generator_lr\": 5e-4,\n","    \"discriminator_lr\": 2e-4,\n","    \"batch_size\": 32,\n","    \"alpha\": 4e-4\n","}"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:57.139886Z","iopub.status.busy":"2024-05-14T13:49:57.139617Z","iopub.status.idle":"2024-05-14T13:49:57.145530Z","shell.execute_reply":"2024-05-14T13:49:57.144560Z","shell.execute_reply.started":"2024-05-14T13:49:57.139862Z"},"trusted":true},"outputs":[],"source":["trainer = Trainer(generator, context_discriminator, config, \n","                 train_dataset, val_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T13:49:57.146879Z","iopub.status.busy":"2024-05-14T13:49:57.146619Z"},"trusted":true},"outputs":[],"source":["trainer.train(epochs=15, train_disc=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images, masks, masks_coords = next(iter(trainer.val_loader))\n","images, masks, masks_coords = images.to(device), masks.to(device), masks_coords.to(device)\n","masked_img = images - images * masks + FILL_VALUE * masks\n","gen_input = torch.cat((masked_img, masks), dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generator.eval()\n","with torch.no_grad():\n","    out = generator(gen_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images = images.detach().cpu()\n","out = out.detach().cpu()\n","masks = masks.detach().cpu()\n","masks_coords = masks_coords.detach().cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_rows, n_cols = 2, 4\n","n_images = 8\n","plt.figure(figsize=(10, 5))\n","for i in range(n_images):\n","    image = images[i].permute(1, 2, 0).numpy()\n","    mask = masks[i].permute(1, 2, 0).numpy()\n","    plt.subplot(n_rows, n_cols, i+1)\n","    plt.imshow(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_rows, n_cols = 2, 4\n","n_images = 8\n","plt.figure(figsize=(10, 5))\n","for i in range(n_images):\n","    image = out[i].permute(1, 2, 0).numpy()\n","    mask = masks[i].permute(1, 2, 0).numpy()\n","    plt.subplot(n_rows, n_cols, i+1)\n","    plt.imshow(image)\n","    for coords in masks_coords[i]:\n","        rect = Rectangle(coords.numpy()[::-1], MASK_SIZE, MASK_SIZE, linewidth=1, edgecolor='r', facecolor='none')\n","        plt.gca().add_patch(rect)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del generator\n","del context_discriminator\n","torch.cuda.empty_cache()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":547506,"sourceId":998277,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
